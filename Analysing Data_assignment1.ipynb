{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ee932-3b35-418d-a49c-7b7f879a7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from glob import glob\n",
    "#install English language model\n",
    "!spacy download en_core_web_sm\n",
    "!spacy download en_core_web_lg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac7e97-75ea-4ea1-a83c-0296912afd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First analysis: three English texts\n",
    "\n",
    "# define function for reading the text files\n",
    "def read_file(filename):\n",
    "    # Read the contents of `filename` and return as a string\n",
    "    with open(filename, encoding='utf8') as infile:\n",
    "        contents = infile.read()\n",
    "    return contents\n",
    "\n",
    "\n",
    "# store the content of the text files in a dictionary (file's name as KEY and content as VALUE)\n",
    "corpus = {}\n",
    "# iterate through all text files, read, and return into dictionary\n",
    "for filename in glob('Part_I_1-2/*.txt'):\n",
    "    corpus[filename] = read_file(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6638684-995a-40be-99be-983ed9bea174",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b45a39-be1e-422d-905c-af7cef58dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from nltk library\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01c8b1-923d-4c52-b226-4ba23d8a2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part I.1: sentence splitting and word tokenization \n",
    "\n",
    "# define function for sentence splitting\n",
    "def split_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# define function for word tokenization\n",
    "def tokenize_words(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    cleaned_tokens = []\n",
    "    punctuation = \".,?!:;()[]''``*\\\"\"\n",
    "    for token in tokens:\n",
    "        # Convert to lowercase and check if it's not punctuation\n",
    "        if token.lower() not in punctuation:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "# define function for removing .txt extension\n",
    "def remove_ext(filepath):\n",
    "    name, ext = os.path.splitext(filepath)\n",
    "    return name\n",
    "\n",
    "# store the preprocessed texts in a dictionary \n",
    "processed_texts = {}\n",
    "# add tokenized sentences and words to the processed_texts dictionary\n",
    "for filename, text in corpus.items():\n",
    "    clean_filename = remove_ext(filename)\n",
    "    sentences = split_sentences(text)  \n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_words = tokenize_words(sentence)\n",
    "        words.extend(tokenized_words)\n",
    "    processed_texts[clean_filename] = {'sentences': sentences, 'words': words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b6848-1e5c-4df7-8453-a31d4700fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ba48e-330a-4471-a7d3-536907bcd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word frequency (per story and in total)\n",
    "\n",
    "# create dictionary for storing word frequencies per story and in total\n",
    "word_frequencies = {}\n",
    "# initialize list to collect all words from all files\n",
    "all_words = []\n",
    "\n",
    "# Calculate word frequencies for each file\n",
    "for filename, data in processed_texts.items():\n",
    "    # Create a nested dictionary for each file's word frequencies\n",
    "    word_frequencies[filename] = {}\n",
    "    \n",
    "    for word in data['words']:\n",
    "        if word not in word_frequencies[filename]:\n",
    "            word_frequencies[filename][word] = 0\n",
    "        word_frequencies[filename][word] += 1\n",
    "    \n",
    "    # Collect all words from all files for total frequency calculation\n",
    "    all_words.extend(data['words'])\n",
    "\n",
    "# Calculate total word frequencies across all files\n",
    "word_frequencies['total'] = {}\n",
    "for word in all_words:\n",
    "    if word not in word_frequencies['total']:\n",
    "        word_frequencies['total'][word] = 0\n",
    "    word_frequencies['total'][word] += 1\n",
    "\n",
    "print(f\"Word frequencies text 1: {word_frequencies['Part_I_1-2/01']}\\n\")\n",
    "print(f\"Word frequencies text 2: {word_frequencies['Part_I_1-2/02']}\\n\")\n",
    "print(f\"Word frequencies text 3: {word_frequencies['Part_I_1-2/03']}\\n\")\n",
    "print(f\"Word frequencies text 4: {word_frequencies['Part_I_1-2/04']}\\n\")\n",
    "print(f\"Word frequencies text 5: {word_frequencies['Part_I_1-2/05']}\\n\")\n",
    "print(f\"Word frequencies in total: {word_frequencies['total']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f913692-fffd-4e69-a543-eb8d860dc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part I.2: plot the 25 most frequent words (per text and in total) in a histogram\n",
    "\n",
    "for story_name, freq_dict in word_frequencies.items():\n",
    "    # Convert dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(freq_dict.items()), columns=['Word', 'Frequency'])\n",
    "    # sort values in descending order and select the top 25 mfw\n",
    "    df = df.sort_values(by='Frequency', ascending=False).head(25)\n",
    "    # create a bar plot\n",
    "    ax = df.plot.bar(x='Word', y='Frequency')\n",
    "    # set the title\n",
    "    ax.set_title(f\"Top 25 Words in {story_name}\")\n",
    "    # set x axis label \n",
    "    ax.set_xlabel(\"Words\")\n",
    "    #set y axis label\n",
    "    ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de927b28-4b1f-4e78-bc29-cef0253e0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Porter and Lancaster modules from nltk.stem package\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8eb68-92dc-4a60-a7f9-487665bb7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step four: stemming \n",
    "\n",
    "# initialize Porter and Lancaster stemmers\n",
    "stemmer_p = PorterStemmer()\n",
    "stemmer_l = LancasterStemmer()\n",
    "\n",
    "# define functions to retrieve stemmed words \n",
    "def stem_words_porter(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer_p.stem(word)\n",
    "        stemmed_words.append(stemmed_word)\n",
    "    return stemmed_words\n",
    "\n",
    "def stem_words_lancaster(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer_l.stem(word)\n",
    "        stemmed_words.append(stemmed_word)\n",
    "    return stemmed_words\n",
    "    \n",
    "# store the stemmed texts into separate dictionaries \n",
    "stemmed_texts_porter = {}\n",
    "stemmed_texts_lancaster = {}\n",
    "\n",
    "# apply both methods for each file's tokenized words\n",
    "for filename, data in processed_texts.items():\n",
    "    words = data['words']   \n",
    "    stemmed_porter = stem_words_porter(words)\n",
    "    stemmed_texts_porter[filename] = stemmed_porter\n",
    "    stemmed_lancaster = stem_words_lancaster(words)\n",
    "    stemmed_texts_lancaster[filename] = stemmed_lancaster\n",
    "\n",
    "print(\"Porter Stemmer Results:\")\n",
    "for filename, stems in stemmed_texts_porter.items():\n",
    "    print(f\"{filename}: {stems}\")\n",
    "\n",
    "print(\"Lancaster Stemmer Results:\")\n",
    "for filename, stems in stemmed_texts_lancaster.items():\n",
    "    print(f\"{filename}: {stems}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8e118-8827-48f6-9fdf-f3bde8dac3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step five: calculate word frequency and plot the 25 mfw for stems\n",
    "\n",
    "# create dictionary for storing word frequencies for Porter stemmer\n",
    "word_frequencies_porter = {}\n",
    "# initialize list to collect all Porter-stemmed words\n",
    "all_stemmed_words_porter = []  \n",
    "\n",
    "# calculate word frequencies for each text file with Porter-stemmed words\n",
    "for filename, stemmed_words in stemmed_texts_porter.items():\n",
    "    word_frequencies_porter[filename] = {}\n",
    "    \n",
    "    for word in stemmed_words:\n",
    "        if word not in word_frequencies_porter[filename]:\n",
    "            word_frequencies_porter[filename][word] = 0\n",
    "        word_frequencies_porter[filename][word] += 1\n",
    "    \n",
    "    # collect all words for total frequency calculation\n",
    "    all_stemmed_words_porter.extend(stemmed_words)\n",
    "\n",
    "# calculate total word frequencies in all text files\n",
    "word_frequencies_porter['total'] = {}\n",
    "for word in all_stemmed_words_porter:\n",
    "    if word not in word_frequencies_porter['total']:\n",
    "        word_frequencies_porter['total'][word] = 0\n",
    "    word_frequencies_porter['total'][word] += 1\n",
    "\n",
    "print(f\"Word frequencies Porter text 1: {word_frequencies_porter['Part_I_1-2/01']}\\n\")\n",
    "print(f\"Word frequencies Porter text 2: {word_frequencies_porter['Part_I_1-2/02']}\\n\")\n",
    "print(f\"Word frequencies Porter text 3: {word_frequencies_porter['Part_I_1-2/03']}\\n\")\n",
    "print(f\"Word frequencies Porter text 4: {word_frequencies_porter['Part_I_1-2/04']}\\n\")\n",
    "print(f\"Word frequencies Porter text 5: {word_frequencies_porter['Part_I_1-2/05']}\\n\")\n",
    "print(f\"Word frequencies Porter in total: {word_frequencies_porter['total']}\\n\")\n",
    "\n",
    "# create dictionary for storing word frequencies for Lancaster stemmer\n",
    "word_frequencies_lancaster = {}\n",
    "# initialize list to collect all words Lancaster-stemmed words\n",
    "all_stemmed_words_lancaster = []  \n",
    "\n",
    "\n",
    "for filename, stemmed_words in stemmed_texts_lancaster.items():\n",
    "    word_frequencies_lancaster[filename] = {}\n",
    "    \n",
    "    for word in stemmed_words:\n",
    "        if word not in word_frequencies_lancaster[filename]:\n",
    "            word_frequencies_lancaster[filename][word] = 0\n",
    "        word_frequencies_lancaster[filename][word] += 1\n",
    "      \n",
    "    all_stemmed_words_lancaster.extend(stemmed_words)\n",
    "\n",
    "\n",
    "word_frequencies_lancaster['total'] = {}\n",
    "for word in all_stemmed_words_lancaster:\n",
    "    if word not in word_frequencies_lancaster['total']:\n",
    "        word_frequencies_lancaster['total'][word] = 0\n",
    "    word_frequencies_lancaster['total'][word] += 1\n",
    "\n",
    "print(f\"Word frequencies Lancaster text 1: {word_frequencies_lancaster['Part_I_1-2/01']}\\n\")\n",
    "print(f\"Word frequencies Lancaster text 2: {word_frequencies_lancaster['Part_I_1-2/02']}\\n\")\n",
    "print(f\"Word frequencies Lancaster text 3: {word_frequencies_lancaster['Part_I_1-2/03']}\\n\")\n",
    "print(f\"Word frequencies Lancaster text 4: {word_frequencies_lancaster['Part_I_1-2/04']}\\n\")\n",
    "print(f\"Word frequencies Lancaster text 5: {word_frequencies_lancaster['Part_I_1-2/05']}\\n\")\n",
    "print(f\"Word frequencies Lancaster total: {word_frequencies_lancaster['total']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c58dc-f6f9-4020-96c7-78789f585d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 25 mfs for Porter\n",
    "for story_name, freq_dict in word_frequencies_porter.items():\n",
    "    df = pd.DataFrame(list(freq_dict.items()), columns=['Stem', 'Frequency'])\n",
    "    df = df.sort_values(by='Frequency', ascending=False).head(25)\n",
    "    ax = df.plot.bar(x='Stem', y='Frequency')\n",
    "    ax.set_title(f\"Top 25 Stems (Porter) in {story_name}\")\n",
    "    ax.set_xlabel(\"Stems\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feda507-c4e4-4268-ab1f-c9b6c851e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 25 mfs for Lancaster\n",
    "for story_name, freq_dict in word_frequencies_lancaster.items():\n",
    "    df = pd.DataFrame(list(freq_dict.items()), columns=['Stem', 'Frequency'])\n",
    "    df = df.sort_values(by='Frequency', ascending=False).head(25)\n",
    "    ax = df.plot.bar(x='Stem', y='Frequency')\n",
    "    ax.set_title(f\"Top 25 Stems (Lancaster) in {story_name}\")\n",
    "    ax.set_xlabel(\"Stems\")\n",
    "    ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42425b-e2c9-47ee-b9c6-1034835b2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part I.3: translations of \"The Adventures of Tom Sawyer\"\n",
    "# import German and Dutch language models\n",
    "# load all three language models\n",
    "!spacy download de_core_news_sm\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "!spacy download nl_core_news_sm\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a97e7c-d66b-4c4d-83e0-60b79c658245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to folders containing the different texts \n",
    "\n",
    "file_path_en = 'Part_I_3/pg74.txt'\n",
    "file_path_de = 'Part_I_3/pg30165.txt'\n",
    "file_path_nl = 'Part_I_3/pg18381.txt'\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, encoding='utf8') as infile:\n",
    "        contents = infile.read()\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c9e02-2c23-420b-876b-656b116f32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_en = 'Part_I_3/pg74.txt'\n",
    "text_en = read_file(file_path_en)\n",
    "# clean file from preamble, TOC, licensing information\n",
    "\n",
    "# split at PREFACE first, and, once removed, split at CHAPTER I and keep everything coming after\n",
    "text_en = text_en.split(\"PREFACE\", 1)[-1]\n",
    "text_en = text_en.split(\"CHAPTER I\", 1) [-1]\n",
    "# split at *** END OF THE PROJECT GUTENBERG EBOOK and keep everything coming before \n",
    "text_en = text_en.split(\"*** END OF THE PROJECT GUTENBERG EBOOK\", 1)[0]\n",
    "\n",
    "# strip whitespace from text\n",
    "cleaned_text_en = text_en.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "cleaned_text_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185729b2-5272-4ca4-bfed-6a4dc2b49515",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_de = 'Part_I_3/pg30165.txt'\n",
    "text_de = read_file(file_path_de)\n",
    "\n",
    "text_de = text_de.split(\"Erstes Kapitel.\", 1)[-1]\n",
    "text_de = text_de.split(\"*** END OF THE PROJECT GUTENBERG EBOOK\", 1)[0]\n",
    "\n",
    "cleaned_text_de = text_de.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "cleaned_text_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bff7f7-e106-48af-bdb9-e317b7b21b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_nl = 'Part_I_3/pg18381.txt'\n",
    "text_nl = read_file(file_path_nl)\n",
    "\n",
    "text_nl = text_nl.split(\"HOOFDSTUK I.\", 1)[1]\n",
    "text_nl = text_nl.split(\"AANTEEKENINGEN\", 1)[0]\n",
    "\n",
    "cleaned_text_nl = text_nl.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "cleaned_text_nl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88ce1f-39ca-408d-b652-d7ce524ba70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and extract POS tags from the English text\n",
    "\n",
    "# load suitable language model\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "# Process the cleaned English text with the language model\n",
    "doc = nlp_en(cleaned_text_en)\n",
    "for token in doc:\n",
    "    # Print each token and its corresponding part-of-speech tag\n",
    "    print(token, \"-\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfe7b2-d147-4715-988b-f02dd16c4744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize and extract POS tags from the German translation\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp_de(cleaned_text_de)\n",
    "for token in doc:\n",
    "    print(token, \"-\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396de93-4612-4579-81c5-a4d3082da85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize and extract POS tags from the Dutch translation\n",
    "\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")\n",
    "doc = nlp_nl(cleaned_text_nl)\n",
    "for token in doc:\n",
    "    print(token, \"-\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699a98c-c398-4f08-945d-f1c5f74cedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the frequencies of the tags for the three languages- English \n",
    "# use built-in spacy function from the documentation (doc.count_by) for counting the occurrences of each POS tag\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp_en(cleaned_text_en)\n",
    "pos_counts = doc.count_by(POS)\n",
    "\n",
    "# convert integer IDs of POS tags to readable tags and sort by frequency\n",
    "for pos, count in sorted(pos_counts.items()):\n",
    "    readable_tag = doc.vocab[pos].text\n",
    "    print(f\"{readable_tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ab256-27e2-4d77-b680-222d9dcf5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the frequencies of the tags for the three languages- German \n",
    "\n",
    "from spacy.attrs import POS\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp_de(cleaned_text_de)\n",
    "pos_counts = doc.count_by(POS)\n",
    "\n",
    "# convert integer IDs of POS tags to readable tags and sort by frequency\n",
    "for pos, count in sorted(pos_counts.items()):\n",
    "    readable_tag = doc.vocab[pos].text\n",
    "    print(f\"{readable_tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f20ff3-8fc2-408f-acc4-8197225a85e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Report the frequencies of the tags for the three languages- Dutch\n",
    "\n",
    "from spacy.attrs import POS\n",
    "\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")\n",
    "doc = nlp_nl(cleaned_text_nl)\n",
    "pos_counts = doc.count_by(POS)\n",
    "\n",
    "# convert integer IDs of POS tags to readable tags and sort by frequency\n",
    "for pos, count in sorted(pos_counts.items()):\n",
    "    readable_tag = doc.vocab[pos].text\n",
    "    print(f\"{readable_tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a7d1b-abd9-44cc-8d50-ed87fa39ee38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 2: perform Named Entity Recognition on texts from Part_I_1-2\n",
    "from spacy import displacy \n",
    "\n",
    "# Load the large language model for better performance and accuracy\n",
    "nlp_en_lg = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, encoding='utf8') as infile:\n",
    "        contents = infile.read()\n",
    "    return contents\n",
    " \n",
    "file_path_1 = 'Part_I_1-2/01.txt'\n",
    "text_1 = read_file(file_path_1)\n",
    "cleaned_text_1 = text_1.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "doc = nlp_en_lg(cleaned_text_1)\n",
    "\n",
    "# use command from the displacy library to visualize the entity recognizer \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696c7a7-8973-45e8-9acc-1028caed62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for all texts \n",
    "file_path_2 = 'Part_I_1-2/02.txt'\n",
    "text_2 = read_file(file_path_2)\n",
    "cleaned_text_2 = text_2.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "doc = nlp_en_lg(cleaned_text_2)\n",
    " \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfb45e-5c53-4a39-8ad7-01c8a9a3915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_3 = 'Part_I_1-2/03.txt'\n",
    "text_3 = read_file(file_path_3)\n",
    "cleaned_text_3 = text_3.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "doc = nlp_en_lg(cleaned_text_3)\n",
    " \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f111fb-893e-4b2c-84b8-06088aca77e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path_4 = 'Part_I_1-2/04.txt'\n",
    "text_4 = read_file(file_path_4)\n",
    "cleaned_text_4 = text_4.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "doc = nlp_en_lg(cleaned_text_4)\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a12bef-3616-4ad8-b4ec-d43901e9d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_5 = 'Part_I_1-2/05.txt'\n",
    "text_5 = read_file(file_path_5)\n",
    "cleaned_text_5 = text_5.replace('\\n', ' ').replace('\\u2028', ' ').strip()\n",
    "doc = nlp_en_lg(cleaned_text_5)\n",
    " \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fa3a3-7333-447c-8bc0-7172ded5e17e",
   "metadata": {},
   "source": [
    "From text 1\n",
    "# Manual annotation\n",
    "[Missy]_PERSON was sitting on a chair in a [house]_FAC, maps and papers spread around, normally her planning was mental, but [River]_PERSON had suggested a physical map to refer to and it was helpful.\n",
    "# Automatic annotation\n",
    "[Missy]_PERSON was sitting on a chair in a house, maps and papers spread around, normally her planning was mental, but [River]_LOC had suggested a physical map to refer to and it was helpful.\n",
    "\n",
    "From text 2\n",
    "# Manual annotation\n",
    "She laughed and smiled. She wished it could have been him after he knew who she was, because what he was mostly talking about was [1969]_DATE , she wanted to know how her parents were in [New York]_GPE , she wanted the more mature version of him, but she lent against him, sitting in the garden, gently pulling him up to show him the [three]_CARDINAL children she had been left with, it wasn’t night yet and they were playing together. \n",
    "# Automatic annotation \n",
    "She laughed and smiled. She wished it could have been him after he knew who she was, because what he was mostly talking about was [1969]_DATE , she wanted to know how her parents were in [New York]_GPE , she wanted the more mature version of him, but she lent against him, sitting in the garden, gently pulling him up to show him the [three]_CARDINAL children she had been left with, it wasn’t night yet and they were playing together. \n",
    "\n",
    "From text 3\n",
    "# Manual annotation\n",
    "Slowly, the [Bad Wolf]_ENTITY held [Rose]_PERSON's hand out, and [the Doctor]_PERSON took it. She stepped to him, inches apart. 'We will make you a deal. The [TARDIS]_PRODUCT, the [Vortex]_LOC, and I'.\n",
    "# Automatic annotation\n",
    "Slowly, the Bad Wolf held [Rose]_PERSON's hand out, and the Doctor took it. She stepped to him, inches apart. 'We will make you a deal. The [TARDIS]_ORG, the [Vortex]_LOC, and [I.\" Rose]_ORG \n",
    "\n",
    "\n",
    "From text 4\n",
    "# Manual annotation\n",
    "Then [The Doctor]_PERSON was repairing some of the wiring under the console. He had recently noticed that the emergency system buttons weren’t translating for [Rose]_PERSON correctly. The [TARDIS]_PRODUCT wasn’t meant to translate [Gallifreyan]_LANGUAGE to other languages.\n",
    "# Automatic annotation\n",
    "Then The Doctor was repairing some of the wiring under the console. He had recently noticed that the emergency system buttons weren’t translating for [Rose]_PERSON correctly. The TARDIS wasn’t meant to translate [Gallifreyan]_PRODUCT to other languages.\n",
    "\n",
    "\n",
    "From text 5\n",
    "# Manual annotation\n",
    "You met his kind with [the Doctor]_PERSON a [few months back]_DATE. They were called [Torwash]_NORP  or [Torvash]_NORP  , you weren’t sure. You may or may not have set off a couple of explosives to blow up their building on planet [Serentara]_LOC , ruining their plans to enslave its population.\n",
    "# Automatic annotation\n",
    "You met his kind with the Doctor a few months back. They were called [Torwash]_PERSON  or [Torvash]_PERSON  , you weren’t sure. You may or may not have set off a couple of explosives to blow up their building on planet [Serentara]_ORG , ruining their plans to enslave its population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccd488-5747-44fc-8057-f228ab697b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Precision, Recall and F1 Score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = ['PERSON', 'FAC', 'PERSON', 'DATE', 'GPE', 'CARDINAL', 'ENTITY', 'PERSON', 'PERSON', 'PRODUCT', 'LOC', '', 'PERSON', 'PERSON', 'PRODUCT', 'LANGUAGE', 'PERSON', 'DATE', 'NORP', 'NORP', 'LOC']\n",
    "y_pred = ['PERSON', '', 'LOC', 'DATE', 'GPE', 'CARDINAL', '', 'PERSON', '', 'ORG', 'LOC', 'ORG', '', 'PERSON', '', 'PRODUCT', '', '', 'PERSON', 'PERSON', 'ORG']\n",
    "target_names = ['PERSON', 'FAC', 'DATE', 'GPE', 'CARDINAL', 'ENTITY', 'PRODUCT', 'LOC', 'LANGUAGE', 'NORP', 'ORG', '']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, zero_division = 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
